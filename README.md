# Evaluating LLMs

Prompts and evaluations for generative code with OpenAI API (CS383 at UMass Amherst)

## Overview

This repository contains materials for evaluating large language models (LLMs) in the context of code generation. It is primarily used in the CS383 course at UMass Amherst. The repo includes Jupyter notebooks with prompts, evaluation scripts, and experiments using the OpenAI API.

## Features

- **Jupyter Notebooks:** All code and experiments are organized as notebooks for easy exploration and reproducibility.
- **Prompts for LLMs:** Example prompts designed to test and compare various LLMs for code generation.
- **Evaluation Scripts:** Notebooks and methods for assessing the quality of code generated by LLMs.

## Getting Started

### Prerequisites

- Python 3.7+
- [Jupyter Notebook](https://jupyter.org/)
- [OpenAI Python SDK](https://github.com/openai/openai-python)

### Installation

1. Clone this repository:
    ```bash
    git clone https://github.com/yuwatidora/evaluating-llms.git
    cd evaluating-llms
    ```

2. (Optional) Create and activate a virtual environment:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```
3. Launch Jupyter Notebook:
    ```bash
    jupyter notebook
    ```

## Usage

- Open any notebook in the repo and follow the instructions within.
- Update the OpenAI API key or relevant credentials as needed for your environment.
- Modify or extend prompts to experiment with different code generation tasks.

## Directory Structure

```
.
├── analysis/                # analyzing data, data cleaning, and data exploration
├── data/                    # Sample input/output data (if any)
├── OpenAI_Prompting.ipynb   # prompts, output parsing and evaluation
└── README.md                # Project overview
```
## Evaluation Method

The evaluation of LLM-generated code in this project is conducted in several structured steps:

1. **Parsing Instructions and Outputs**
    - Each prompt (instruction) and the corresponding model-generated output are parsed:
        - The instruction is analyzed to extract input parameters and the expected output.
        - The model output is scanned for Python code blocks.

2. **Code Extraction**
    - The extracted code is isolated from the model’s output using regular expressions, focusing on code blocks demarcated with ```python ... ``` or similar markers.

3. **Automated Code Execution**
    - Each extracted code snippet is executed in a sandboxed subprocess using the parsed input parameters.
    - The execution is strictly time-limited to avoid infinite loops or long runtimes (e.g., a 2-second timeout).
    - Only the function defined in the generated code is called, using the input parameters parsed from the instruction.

4. **Output Comparison**
    - The output produced by the code is compared to the expected output from the instruction.
    - Comparison is type-aware:
        - Strings are normalized (case, whitespace).
        - Lists and dictionaries are compared structurally.
        - Booleans and numbers are compared by value.

5. **Error Handling**
    - If the code execution results in an error (exceptions, timeouts, or missing code), this is logged.
    - These cases are counted as failures in the final evaluation.

6. **Aggregation and Reporting**
    - For each prompt/code pair, the result is categorized as:
        - **Correct** (output matches expected)
        - **Incorrect** (output does not match)
        - **Error** (exception, timeout, or unparsable code)
    - The overall proportion of correct, incorrect, and error cases is summarized to assess the LLM’s code generation accuracy.

This method enables large-scale, fully automated, and reproducible benchmarking of code-generation capabilities of LLMs by directly testing generated code against expected results in a controlled environment.

## Findings

This project filtered a large Python instruction dataset and used OpenAI’s GPT-4o-mini to generate code solutions for 500 unique prompts. The workflow included parsing instructions/outputs, extracting code, and running automated evaluation.

**Key findings:**

- The pipeline efficiently filtered and prepared high-quality prompt data, ensuring relevance and diversity.
- The LLM was able to generate executable Python code for the majority of prompts.
- Automated evaluation revealed:
  - Most generated solutions matched the expected outputs.
  - About 5% of cases had missing or unparsable code blocks.
  - Some generated code failed due to exceptions or timeouts, highlighting common LLM failure modes.
- The approach enables large-scale, reproducible benchmarking of code generation models.

**Limitations and next steps:**
- Handling of edge cases (e.g., ambiguous instructions, non-Python outputs) could be improved.
- Future work could include more detailed error categorization, human-in-the-loop evaluation, and comparison across multiple LLMs.

## License

This project is licensed under the MIT License.

## Acknowledgments

- UMass Amherst CS383
- OpenAI for providing API access
